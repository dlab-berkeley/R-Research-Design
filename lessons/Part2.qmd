---
title: "Declaring a Design with DeclareDesign"
format: html
editor: visual
---

Alright, we have a research idea and some ideas of how it might fit into the MIDA framework. Before we actually embark on the design, we often want to know whether or not what we have written down will actually work. The R package `DeclareDesign` provides a suite of tools to declare, diagnose, and redesign a research design. `DeclareDesign` is designed around the MIDA framework.

Load the packages required the workshop with the following code.

```{r}
#| echo: false 
#| message: false 
#| warning: false
# install.packages(c("tidyverse", "DeclareDesign"))

library(DeclareDesign)
library(tidyverse)
```

The point of DeclareDesign is to *declare a design*, which means that we declare in code each part of the MIDA framework. There are two advantages to code declarations. The first is that we clearly specify what we are going to do. While that is valuable for the reason that the previous exercise is valuable, putting down our ideas in code gives us another advantage. We can *diagnose* the design to determine whether it actually will achieve what we want it to do. For example, we can find out ahead of time whether the number of subject we have in mind for an experiment is large enough to be confident in the answer.

## Declaring a Model

We start by declaring a model with `declare_model()`. The function takes a set of declarations about the model context.

Let's declare some models! Here is a model `M` with 100 subjects that have a measured characteristic normally distributed with a mean of 0 and a standard deviation of 1 and an binary outcome variable Y that depends on the realization of X and an unobserved covariate U.

```{r}
M = declare_model(N = 100,
                  X = rnorm(N,0,1),
                  U = runif(N, 0,1),
                  Y = if_else(X + U > 0.5, 1,0)
                  )
```

When we call M as a function, we get back a data frame of the features of the population.

```{r}
## Print just the first 6 rows.
head(M())
```

### Modeling Potential Outcomes

When we consider counterfactual quantities, we need to declare the potential outcomes. We can do this in two ways. The first is to explicitly create our potential outcomes exactly as we did with other variables.

```{r}
PO_example1 = declare_model(
  N = 100,
  Y_Z_0 = rbinom(N, size = 1, prob = 0.5),
  Y_Z_1 = rbinom(N, size = 1, prob = 0.5)
)
```

Alternatively, we can use the `potential_outcomes()` helper function to do the same thing with a formula syntax. Here's an example of using the function to create model with a treatment effect of 0.1.

```{r}
## We put in the .2 
PO_example2 = declare_model(
  N = 100,
  Noise = rnorm(N),
  potential_outcomes(Y ~ 0.1*Z + Noise,
                     conditions = list(Z = c(0,1)))
)
```

The `conditions` argument in the function shows us all the values that Z could take. Here we have a list with just one condition, but we can list multiple "conditions" as well.

#### Challenge

Amend the previous declaration so that Z can take on four values, 25, 50, 75, 100. What does the resulting data frame output look like?

```{r}
M_4Z = ... ## replace ... with answer

```

### Challenge

A type of experiment known as a factorial design randomly assigns multiple treatments. The standard design is a 2x2 factorial designs where there are two treatments and each treatment has two levels. Imagine that there is a treatment effect of **1** for the first treatment, **2** for the second, and **10** for the interaction. Declare the model with a population of size 100.

```{r}
M_factorial = ...

```

## Inquiry

Our inquiry is the summary function of the events generated by our model declaration. There are two common types of inquiries: descriptive and causal.

Here is an example of a causal inquiry. We want to write the inquiry in context of the variables that we declared in our model declaration. We have rewritten our model declaration to make it easy to recall.

```{r}
M = declare_model(
  N = 100,
  Noise = rnorm(N),
  X = rbinom(N, 1, prob = 0.5),
  potential_outcomes(Y ~ 0.1*Z + 0.2*X + Noise,
                     conditions = list(Z = c(0,1)))
)
## New! 
I = declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))
```

Note that we define our inquiry as an argument, and tell the function what it is supposed to do. Here we define the **Average Treatment Effect (ATE)** which is the difference between the potential outcome under the treatment condition and the potential outcome under control.

In many experiments we are often interested in multiple inquiries. For example, in a drug trial we might be interested in the ATE of a new drug, but also its effect on younger and older patients. The latter is an example of a **Conditional Average Treatment Effect (CATE)** because we want to know the average effect conditional on a variable (here age). We can declare multiple inquiries at once.

```{r}
I_mult = declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0),
                         CATE_X0 = mean(Y_Z_1[X==1] - Y_Z_0[X==1]))
```

#### Challenge

Post an example of an ATE in your research domain in the chat. Can you think of a conditional average treatment effect that might be interesting?

### Data Strategy

A data strategy describes the sampling strategy for units, the assignment of treatment conditions, and the way to measure outcomes.

Here we declare a complete random assignment and a "reveal" the potential outcomes according to the treatment assignments. The `reveal_outcomes()` functions picks out the right potential outcome to reveal for each unit. In words, if a unit is in the `Z==1` group, the code below will reveal their potential outcome under treatment `Y_Z_1`.

```{r}
D = declare_assignment(
  Z = complete_ra(N, m = N/2)
  ) + 
  declare_measurement(Y = reveal_outcomes(Y~Z))
```

The `+` between the two functions is a way to chain the two functions together. `DeclareDesign` supports chaining, and a full design often is a set of chained calls together.

### Answer Strategy

Let's review in one place our design strategy.

```{r}
M = declare_model(
  N = 100,
  Noise = rnorm(N),
  X = rbinom(N, 1, prob = 0.5),
  potential_outcomes(Y ~ 0.1*Z + 0.2*X + Noise,
                     conditions = list(Z = c(0,1)))
)
## New! 
I = declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))
D = declare_assignment(
  Z = complete_ra(N, m = N/2)
  ) + 
  declare_measurement(Y = reveal_outcomes(Y~Z))
```

The last part of MIDA that we need to add is our answer strategy. The answer strategy is a function that provides an answer, for example a linear regression. The following code completes our design and shows how to chain all the calls together.

```{r}
Example_Declaration = declare_model(
  N = 1000,
  Noise = rnorm(N),
  potential_outcomes(Y ~ 0.1*Z + Noise,
                     conditions = list(Z = c(0,1)))
) + 
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) +
  declare_assignment(
    Z = complete_ra(N, m = N/2)
  ) + 
  declare_measurement(Y = reveal_outcomes(Y~Z)) +
  ## New 
  declare_estimator(
    Y ~ Z,
    .method = lm_robust,
    .summary = tidy,
    term = "Z",
    inquiry = "ATE", 
    label = "Regression"
  )
```

Let's break down these arguments:

-   The first argument in the `declare_estimator` function defines what we want to estimate. Here it is a regression function that not coincidentally matches the known data generating process. *What might happen if we did not include X?*

-   The `.method` argument defines what method we will use to get that estimate. Here it is the robust linear model specification from the `estimatr` package, but we could use just about any modeling function in R.

-   The `.summary` argument tells us what summary function we want to extract our coefficient estimates with. `DeclareDesign` plays nice with the tidyverse, so we often use the tidy option as a default. If we wanted the model fit statistics, we would use `glance` instead of `tidy` as the value.

-   The `term` argument tells us what term in the regression we are interested in. Here that is the `Z` coefficient value.

-   The last two arguments are for labeling what we did.

### Diagnosis

Now that we have all the code declared, the diagnosis step is simple. We simply pass our declaration off to `diagnose_design()` which does all the work for us.

```{r}
# The simulation may take a second or two to run.
# Here we are using R's built in pipe operator to 
# pass our declaration to the diagnose_design function
diagnosis = Example_Declaration |>
  diagnose_design(
    sims = 2000,
    bootstrap_sims = 100 
  )

```

We get back a data frame from this call. If we want to make plots of the simulations, we can work with them directly like so:

```{r}
sims_d = get_simulations(diagnosis = diagnosis)
```

Note that `DeclareDesign` plays nicely with the tidyverse, so we can use tidy tools to summarize results. For example, here's some code that shows the bias of each simulation.

```{r}
## Get the bias 
bias_summary = sims_d |>
  group_by(estimator) |>
  summarize(estimand = mean(estimand))

## Plot the simulations 
sims_d |>
  ggplot(aes(x = estimate)) +
  geom_histogram(bins = 40, fill = "lightblue")+
  geom_vline(xintercept = bias_summary$estimand,
             lty = "dashed",
             color = "black")+
  theme_minimal()+
  labs(x = "Estimate",
       y = "Simulation Count",
       title = "Bias Diagnosis")
```

We see from this chart that this design is unbiased, as the chart is centered on the true estimand of 0.1.

#### Challenge

How powerful is our design? We often can show this with a *power curve* where we display the probability of achieving statistical significance along different possible effect sizes. For design purposes, a run is *significant* if it identifies a significant effect at a given threshold, usually when the p.value $\leq$ 0.05, and 0 otherwise. To test this out, we have provided some starter code for a model declaration.

```{r}
#| warning: false
#| message: false
powerful = declare_model(
  N = 200,
  Noise = rnorm(N),
  # This line gets Z's from a uniform distribution of 0 to 1
  potential_outcomes(Y ~ runif(1,0,1)*Z + Noise)
) +
  declare_inquiry(ATE = mean(Y_Z_1 -Y_Z_0)) +
  declare_assignment(Z = complete_ra(N)) +
  declare_measurement(Y = reveal_outcomes(Y~Z)) +
  declare_estimator(Y~Z,
                    .method = lm_robust,
                    .summary = tidy,
                    term = "Z",
                    inquiry = "ATE", 
                    label = "Regression"
                    )

## Diagnose and simulate the design
## Create a variable 'sig' that determines whether a run was significant. mutate() is helpful here

#power_sim = ...

power_sim = diagnose_design(powerful) |>
  get_simulations() |>
  mutate(sig = if_else(p.value <= 0.05, 1, 0))

## Plot code 
power_sim |>
  ggplot(aes(estimand, sig))+
  geom_smooth(method = "loess")+
  geom_hline(yintercept = 0.8,linetype = "dashed")+
  ylim(c(0,1))+
  labs(x = "True Effect Size",
       y = "Diagnosed Statistical Power")
```

### 
